{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Meta Learning\n",
    "\n",
    "## Seoul AI Meetup, September 16\n",
    "\n",
    "Martin Kersner, <m.kersner@gmail.com>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "* train/val/test dataset\n",
    "* bias/variance\n",
    "* discovering meta knowledge/bagging/boosting/stacking/dynamic bias selection/inductive transfer\n",
    "* adaboost, gradient boosting, random forest\n",
    "* netflix, kaggle\n",
    "* nexar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Learning\n",
    "\n",
    "Meta Learning is a way of combining models using Meta algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised Learning only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training, Validation and Test Dataset\n",
    "\n",
    "todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization Error\n",
    "https://en.wikipedia.org/wiki/Generalization_error\n",
    "\n",
    "Generalization error is measure of how accurately an algorithm is able to predict outcome values for previously **unseen data**.\n",
    "\n",
    "\n",
    "Generalization is composed of three parts:\n",
    "\n",
    "* Bias\n",
    "* Variance\n",
    "* **Irreducible Error**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bias, Variance\n",
    "https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\n",
    "\n",
    "* Bias\n",
    "  * Error from erroneous assumptions in the learning algorithm.\n",
    "  * Can cause **underfitting**.\n",
    "  \n",
    "\n",
    "* Variance\n",
    "  * Error from sensitivity to small fluctuations in the training set.\n",
    "  * Can cause **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"https://qph.ec.quoracdn.net/main-qimg-f9c226fe76f482855b6d46b86c76779a\" style=\"height: 50%; width: 50%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bias-Variance Tradeoff\n",
    "\n",
    "<center>\n",
    "<img src=\"http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png\" style=\"height: 50%; width: 50%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ensemble\n",
    "\n",
    "Combining these rules will provide robust prediction as compared to prediction done by individual rules\n",
    "Ensemble model combines multiple ‘individual’ (diverse) models together and delivers superior prediction power.\n",
    "\n",
    "an ensemble is a supervised learning technique for combining multiple weak learners/ models to produce a strong learner. Ensemble model works better, when we ensemble models with low correlation.\n",
    "\n",
    "the random forest algorithm (having multiple CART models). It performs better compared to individual CART model by classifying a new object where each tree gives “votes” for that class and the forest chooses the classification having the most votes (over all the trees in the forest). In case of regression, it takes the average of outputs of different trees.\n",
    "\n",
    "The key to creating a powerful ensemble is model diversity. An ensemble with two techniques that are very similar in nature will perform poorly than a more diverse model set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Voting Ensembles\n",
    "\n",
    "TODO link (ensembling guide MLWave)\n",
    "\n",
    "Voting ensembled works better to ensemble low-correlated model predictions.\n",
    "\n",
    "Majority votes make most sense when the evaluation metric requires hard predictions, for instance with (multiclass-) classification accuracy.\n",
    "\n",
    "Final class is selected based on (weighted) majority voting.\n",
    "\n",
    "* Majority Voting Ensemble\n",
    "* Weighted Voting Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Majority Voting Ensemble Example\n",
    "\n",
    "3 independent binary models (A, B, C) with accuracy 70 %.\n",
    "\n",
    "* 70 % of time correct prediction.\n",
    "* 30 % of time wrong prediction.\n",
    "\n",
    "**At least two predictions (out of three) have to be correct.**\n",
    "\n",
    "\n",
    "Voting Mechanism:\n",
    "> * A: 1\n",
    "> * B: 1\n",
    "> * C: 0\n",
    ">\n",
    "> => 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### All three are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3429999999999999\n"
     ]
    }
   ],
   "source": [
    "P3 = 0.7 * 0.7 * 0.7\n",
    "print(P3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Two are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4409999999999999\n"
     ]
    }
   ],
   "source": [
    "P2 = 3 * (0.7 * 0.7 * 0.3)\n",
    "print(P2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### One is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.189\n"
     ]
    }
   ],
   "source": [
    "P1 = 3 * (0.3 * 0.3 * 0.7)\n",
    "print(P1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### None is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.027\n"
     ]
    }
   ],
   "source": [
    "P0 = 0.3 * 0.3 * 0.3\n",
    "print(P0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "P = P0 + P1 + P2 + P3\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Voting Ensemble Example Result\n",
    "\n",
    "Most of the time (P2 ~ 44 %) the majority vote corrects an error.\n",
    "\n",
    "Prediction accuracy of majority ensembling mode will be **78.38 %** (P3 + P2) which is higher than when using models individually.\n",
    "\n",
    "Using **5** independent binary models with accuracy 70 %, accuracy of majority voting raises to **83.69 %**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Correlation\n",
    "\n",
    "* Pearson Correlation\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For highly correlated models, majority voting enembles don't help much or not at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "GT = np.array([1,1,1,1,1,1,1,1,1,1])\n",
    "\n",
    "A  = np.array([1,1,1,1,1,1,1,1,0,0]) # 80 % accuracy\n",
    "B  = np.array([1,1,1,1,1,1,1,1,0,0]) # 80 % accuracy\n",
    "C  = np.array([1,0,1,1,1,1,1,1,0,0]) # 70 % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Accuracy with voting ensembles is still 80 %!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80000000000000004"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(A+B+C >= 2)/len(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([1,1,1,1,1,1,1,1,0,0]) # 80 % accuracy\n",
    "B = np.array([0,1,1,1,0,1,1,1,0,1]) # 70 % accuracy\n",
    "C = np.array([1,0,0,0,1,0,1,1,1,1]) # 60 % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Using highly uncorrelated models, accuracy raised to 90 %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90000000000000002"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(A+B+C >= 2)/len(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Voting Ensemble\n",
    "\n",
    "To give a better model more weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging\n",
    "\n",
    "works well for a wide range of problems (both classification and regression) and metrics (AUC, squared error or logaritmic loss).\n",
    "\n",
    "Geometric mean\n",
    "\n",
    "reduces overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank Averaging\n",
    "\n",
    "do well when evaluation metric is ranking or threshold based like AUC\n",
    "\n",
    "\n",
    "https://www.kaggle.com/cbourguignat/why-calibration-works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking, Blending\n",
    "\n",
    "### Stacked Generalization\n",
    "The basic idea behind stacked generalization is to use a pool of base classifiers, then using another classifier to combine their predictions, with the aim of reducing the generalization error.\n",
    "\n",
    "**2 fold stacking**\n",
    "\n",
    "Split the train set in 2 parts: train_a and train_b\n",
    "Fit a first-stage model on train_a and create predictions for train_b\n",
    "Fit the same model on train_b and create predictions for train_a\n",
    "Finally fit the model on the entire train set and create predictions for the test set.\n",
    "Now train a second-stage stacker model on the probabilities from the first-stage model(s).\n",
    "\n",
    "A stacker model gets more information on the problem space by using the first-stage predictions as features, than if it was trained in isolation.\n",
    "\n",
    "### Blending (= stacked ensembling)\n",
    "\n",
    "It is very close to stacked generalization, but a bit simpler and less risk of an information leak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ensemble Meta-Algorithms\n",
    "\n",
    "* Bagging\n",
    "* Boosting\n",
    "* Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging (Bootstrap Aggreagting)\n",
    "\n",
    "1. Create **random samples** (sampling uniformly and with replacement) of the training data set.\n",
    "2. Build a model **from each sample**.\n",
    "3. **Combine** results of these multiple classifiers using **average** (regression) or **majority voting** (classification).\n",
    "\n",
    "\n",
    "* Bagging helps to reduce the variance error.\n",
    "* Can be trained in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting\n",
    "\n",
    "**Boosting** is a method of turning set of **weak learners** to one **strong learner**.\n",
    "\n",
    "* Weak learner\n",
    "  * Classifier which can label testing examples better than random guessing.\n",
    "* Strong learner\n",
    "  * Classifier that is arbitrarily well-correlated with the true label.\n",
    "\n",
    "\n",
    "1. Assign weight (same for each example) to each training example.\n",
    "2. Train weak classifier on whole training dataset.\n",
    "3. Evaluate weak classifier and reweight data accordingly:\n",
    "   * Misclassified examples **gain** weight.\n",
    "   * Correctly clasified examples **lose** weight.\n",
    "4. Train another weak classifier that focuses on examples that were misclassified by previous weak learner. \n",
    "5. Evaluate weak clasifier and modify weights appropriately (as in step 3).\n",
    "6. Repeat from 4 unless you achieve required accuracy or reach the maximum number of weak learners.\n",
    "\n",
    "\n",
    "When weak classifiers are put together, they are typically weighted in some way.\n",
    "\n",
    "\n",
    "* Boosting is primarily reducing bias, and also variance.\n",
    "* Tends to overfit the training data.\n",
    "* Can be trained only sequentally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost\n",
    "### Brownboost\n",
    "### xgboost\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stacking (Stacked Generalization)\n",
    "\n",
    "Training a model to combine the **predictions** of several other models.\n",
    "\n",
    "Stacking works in two phases.\n",
    "\n",
    "1. Train several base models using all data (data splitting).\n",
    "2. Train model to make final predictions using predictions results from trained models in the first phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How can we identify the weights of different models for ensemble?\n",
    "\n",
    "ne of the most common challenge with ensemble modeling is to find optimal weights to ensemble base models. In general, we assume equal weight for all models and takes the average of predictions. But, is this the best way to deal with this challenge?\n",
    "\n",
    "There are various methods to find the optimal weight for combining all base learners. These methods provide a fair understanding about finding the right weight. I am listing some of the methods below:\n",
    "\n",
    "Find the collinearity between base learners and based on this table, then identify the base models to ensemble. After that look at the cross validation score (ratio of score) of identified base models to find the weight.\n",
    "Find the algorithm to return the optimal weight for base learners. You can refer article Finding Optimal Weights of Ensemble Learner using Neural Network to look at the method to find optimal weight.\n",
    "We can also solve the same problem using methods like:\n",
    "Forward Selection of learners\n",
    "Selection with Replacement\n",
    "Bagging of ensemble methods\n",
    "You can also look at the winning solution of Kaggle / data science competitions to understand other methods to deal with this challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What are the benefits of ensemble model?\n",
    "\n",
    "There are two major benefits of Ensemble models:\n",
    "\n",
    "* Better prediction\n",
    "* More stable model\n",
    "\n",
    "\n",
    "The aggregate opinion of a multiple models is less noisy than other models. In finance, we called it “Diversification”  a mixed portfolio of many stocks will be much less variable than just one of the stocks alone. This is also why your models will be better with ensemble of models rather than individual. One of the caution with ensemble models are over fitting although bagging takes care of it largely."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
