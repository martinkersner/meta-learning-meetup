{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Meta Learning\n",
    "\n",
    "## Seoul AI Meetup, September 16\n",
    "\n",
    "Martin Kersner, <m.kersner@gmail.com>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References \n",
    "\n",
    "* Ensembling\n",
    "  * https://mlwave.com/kaggle-ensembling-guide/\n",
    "  * https://github.com/ageron/handson-ml/blob/master/07_ensemble_learning_and_random_forests.ipynb\n",
    "* AdaBoost\n",
    "  * http://www.robots.ox.ac.uk/~az/lectures/cv/adaboost_matas.pdf\n",
    "  * http://www.cs.princeton.edu/courses/archive/spr08/cos424/readings/Schapire2003.pdf\n",
    "* Netflix Prize\n",
    "  * http://www.netflixprize.com/\n",
    "  * https://medium.com/netflix-techblog/netflix-recommendations-beyond-the-5-stars-part-1-55838468f429\n",
    "* Kaggle competitions\n",
    "  * https://www.kaggle.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Links To Scripts\n",
    "\n",
    "* https://github.com/MLWave/hodor-autoML/tree/master/stacking/blend_proba\n",
    "* https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/blend_proba.py\n",
    "* https://github.com/emanuele/kaggle_pbr/blob/master/blend.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mathematical Notation\n",
    "\n",
    "* $X$ data, input space\n",
    "* $Y$ labels, output space\n",
    "* $x_i$ is the feature vector of the i-th example\n",
    "* $y_i$ is label (i.e., class) for $x_i$\n",
    "* $m$ number of training examples\n",
    "* $n$ number of features\n",
    "* $D_{j}(i)$ weight of $i$-th training example for $j$-th base learner (AdaBoost)\n",
    "* $E$ erorr function\n",
    "\n",
    "### Technical Terms\n",
    "base learner = weak learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Content \n",
    "\n",
    "* Prerequsities\n",
    "  * Supervised Learning\n",
    "  * Classification, Regression\n",
    "  * Data Splitting\n",
    "  * Bias-Variance Tradeoff, Irreducible error\n",
    "  * Underfitting, Overfitting\n",
    "* Ensembles\n",
    "  * Voting Ensemble\n",
    "  * Ranking\n",
    "* Meta Learning\n",
    "  * Bagging\n",
    "  * Boosting\n",
    "  * Stacking/Blending\n",
    "* Nexar Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Learning\n",
    "\n",
    "https://en.wikipedia.org/wiki/Supervised_learning\n",
    "\n",
    "* $N$ training examples of the form $\\{\\{x_1, y_1\\},\\dotso\\{x_n, y_n\\}\\}$\n",
    "* Searching for a function $g: X \\rightarrow Y$\n",
    "\n",
    "### Classification vs Regression\n",
    "\n",
    "* Regression\n",
    "  * Output variable takes **continuous values**.\n",
    "  * E.g. Price prediction of certain stock.\n",
    "* Classification\n",
    "  * Output variable takes **class labels**.\n",
    "  * E.g. Prediction of what object is in image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Splitting\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/martinkersner/meta-learning-meetup/master/data/train_val_test.jpeg\" style=\"width:70%; height:70%\" />\n",
    "</center>\n",
    "\n",
    "* **Training** dataset is used for training.\n",
    "* **Validation** dataset is used for model evaluation.\n",
    "* **Testing** data imitate real unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Splitting\n",
    "\n",
    "### Cross-validation\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/martinkersner/meta-learning-meetup/master/data/cross_validation.png\" style=\"height:70%; width:70%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalization Error\n",
    "https://en.wikipedia.org/wiki/Generalization_error\n",
    "\n",
    "* Generalization error is measure of how accurately an algorithm is able to predict outcome values for previously **unseen data**.\n",
    "* Generalization error is composed of three parts:\n",
    "  * Bias\n",
    "  * Variance\n",
    "  * Irreducible Error\n",
    "    * Due to noisiness of the data.\n",
    "    * Can be reduced by cleaning the data (not using wrong/inaccurate data points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bias, Variance\n",
    "\n",
    "https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\n",
    "\n",
    "* Bias\n",
    "  * Error from **wrong assumptions** in the learning algorithm.\n",
    "  * Can cause **underfitting**.\n",
    "  \n",
    "\n",
    "* Variance\n",
    "  * Error from **sensitivity to small variations** in the training set.\n",
    "  * Can cause **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bias, Variance\n",
    "\n",
    "<center>\n",
    "<img src=\"https://qph.ec.quoracdn.net/main-qimg-f9c226fe76f482855b6d46b86c76779a\" style=\"height: 50%; width: 50%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias-Variance Tradeoff\n",
    "* **Increasing model complexity** lead to **increase of variance** and **reduction of bias**.\n",
    "* **Reducing model complexity** lead to **increase of bias** and **reduction of variance**.\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png\" style=\"height: 50%; width: 50%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Underfitting, Overfitting\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/martinkersner/meta-learning-meetup/master/data/underfitting_overfitting.png\" style=\"height:90%; width:90%\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ensembles and Meta Learning\n",
    "\n",
    "> https://en.wikipedia.org/wiki/Meta_learning_(computer_science)\n",
    ">\n",
    "> \"Meta learning is a subfield of Machine learning where **automatic learning algorithms** are applied on meta-data about machine learning experiments.\"\n",
    "\n",
    "* Ensembles\n",
    "* Meta-Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ensembles\n",
    "\n",
    "* Majority Voting Ensembles\n",
    "* Weighted Voting Ensemble\n",
    "* Rank Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Voting Ensembles\n",
    "\n",
    "https://mlwave.com/kaggle-ensembling-guide/\n",
    "\n",
    "* Works better with **low-correlated** model predictions.\n",
    "* Good for **hard predictions** (e.g. multiclass classification accuracy)\n",
    "* Final class is selected based on (weighted) majority voting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Voting Ensembles\n",
    "\n",
    "#### Approaches \n",
    "\n",
    "* Majority Voting Ensemble\n",
    "  * Hard Voting\n",
    "  * Soft Voting\n",
    "    * Predict class with the highest class probability, averaged over individual classifiers.\n",
    "    * In *scikit-learn* all weak classifiers need to have implemented ```predict_proba()``` method and ```voting``` parameter of models set to ```True```.\n",
    "* Weighted Voting Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Majority Voting Ensemble\n",
    "\n",
    "#### Example\n",
    "\n",
    "3 independent binary classification models (A, B, C) with accuracy 70 %.\n",
    "\n",
    "* 70 % of time correct prediction.\n",
    "* 30 % of time wrong prediction.\n",
    "\n",
    "At least two predictions (out of three) have to be correct.\n",
    "\n",
    "\n",
    "> **Voting Mechanism**\n",
    "> * A: 1\n",
    "> * B: 1\n",
    "> * C: 0\n",
    ">\n",
    ">\n",
    "> * Final classification: 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Majority Voting Ensemble\n",
    "\n",
    "#### All three are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3429999999999999\n"
     ]
    }
   ],
   "source": [
    "P3 = 0.7 * 0.7 * 0.7\n",
    "print(P3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Two are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4409999999999999\n"
     ]
    }
   ],
   "source": [
    "P2 = 3 * (0.7 * 0.7 * 0.3)\n",
    "print(P2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Majority Voting Ensemble\n",
    "\n",
    "#### One is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.189\n"
     ]
    }
   ],
   "source": [
    "P1 = 3 * (0.3 * 0.3 * 0.7)\n",
    "print(P1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### None is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.027\n"
     ]
    }
   ],
   "source": [
    "P0 = 0.3 * 0.3 * 0.3\n",
    "print(P0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "P = P0 + P1 + P2 + P3\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Majority Voting Ensemble\n",
    "\n",
    "#### Result\n",
    "\n",
    "Most of the time (P2 ~ 44 %) the majority vote corrects an error.\n",
    "\n",
    "Prediction accuracy of majority ensembling mode will be **78.38 %** (P3 + P2) which is higher than when using models individually.\n",
    "\n",
    "Using **5** independent binary models with accuracy 70 %, accuracy of majority voting raises to **83.69 %**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Correlation\n",
    "\n",
    "* **0** no correlation\n",
    "* **+1** positive correlation\n",
    "* **-1** negative correlation\n",
    "\n",
    "### Pearson Correlation\n",
    "https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\n",
    "\n",
    "* **Linear** correlation between **two** variables\n",
    "\n",
    "### Spearman's rank correlation coefficient\n",
    "\n",
    "https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient\n",
    "\n",
    "* **Monotonic** correlation between **two** variables\n",
    "\n",
    "\n",
    "#### Open-source\n",
    "https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/correlations.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Correlated Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "GT = np.array([1,1,1,1,1,1,1,1,1,1])\n",
    "\n",
    "A  = np.array([1,1,1,1,1,1,1,1,0,0]) # 80 % accuracy\n",
    "B  = np.array([1,1,1,1,1,1,1,1,0,0]) # 80 % accuracy\n",
    "C  = np.array([1,0,1,1,1,1,1,1,0,0]) # 70 % accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80000000000000004"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(A+B+C >= 2)/len(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Accuracy with voting ensembles is still **only 80 %**!\n",
    "\n",
    "For highly correlated models, majority voting enembles don't help much or not at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Non-correlated Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([1,1,1,1,1,1,1,1,0,0]) # 80 % accuracy\n",
    "B = np.array([0,1,1,1,0,1,1,1,0,1]) # 70 % accuracy\n",
    "C = np.array([1,0,0,0,1,0,1,1,1,1]) # 60 % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Using **highly uncorrelated models**, accuracy raised to **90 %**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90000000000000002"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(A+B+C >= 2)/len(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Majority Voting Ensemble\n",
    "\n",
    "#### scikit-learn\n",
    "\n",
    "[sklearn.ensemble.VotingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# source: Hands-on Machine Learning with Scikit-Learn & Tensorflow, Chapter 7\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)), ('rf', RandomFor...f',\n",
       "  max_iter=-1, probability=False, random_state=42, shrinking=True,\n",
       "  tol=0.001, verbose=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='hard', weights=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: Hands-on Machine Learning with Scikit-Learn & Tensorflow, Chapter 7\n",
    "log_clf = LogisticRegression(random_state=random_state)\n",
    "rnd_clf = RandomForestClassifier(random_state=random_state)\n",
    "svm_clf = SVC(random_state=random_state)\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf),\n",
    "                                          ('rf', rnd_clf),\n",
    "                                          ('svc', svm_clf)],\n",
    "                              voting='hard') # or soft\n",
    "\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Weighted Voting Ensemble\n",
    "\n",
    "* Weights of individual models in ensemble can differ.\n",
    "* The main purpose is to give more weight to a better model.\n",
    "* E.g. Model with better performance should have larger impact. Low performing models have to overrule (same prediction) high performing model, otherwise their classification result will be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Weighted Voting Ensemble\n",
    "\n",
    "#### Approaches to Weight Selection\n",
    "\n",
    "One of the most common challenge with ensemble modeling is to find optimal weights to ensemble base models.\n",
    "\n",
    "* Same weights for each model\n",
    "* Heuristical approach\n",
    "* Use cross-validation score of base models to estimate weights\n",
    "* Explore Kaggle winning solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Averaging\n",
    "\n",
    "* Works well for a wide range of problems (classification/regression) and metrics (AUC, squared error or logaritmic loss).\n",
    "* Often **reduces overfit** (smoothens separation between classes).\n",
    "\n",
    "\n",
    "* Arithmetic Mean\n",
    "  * https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/kaggle_avg.py\n",
    "* Geometric Mean\n",
    "  * $(\\prod_{i=1}^{n} x_{i})^{\\frac{1}{n}}$\n",
    "  * Good when comparing values with different numeric ranges.\n",
    "  * https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/kaggle_geomean.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Averaging\n",
    "\n",
    "#### Why it works?\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/martinkersner/meta-learning-meetup/master/data/overfit.png\" style=\"height:50%;width:50%\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Rank Averaging\n",
    "\n",
    "https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/kaggle_rankavg.py\n",
    "\n",
    "https://www.kaggle.com/cbourguignat/why-calibration-works\n",
    "\n",
    "* Good for uncalibrated predictors.\n",
    "  * Probability predictions aren't spread over whole range (0.0 - 1.0)\n",
    "* Works well on evaluation metric as ranking or threshold based like AUC.\n",
    "\n",
    "\n",
    "#### Computation\n",
    "1. Turn the predictions into ranks (```np.argmin()```).\n",
    "2. Average these ranks.\n",
    "3. Compute ranks of averages and normalize them to 0 - 1 range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Rank Averaging\n",
    "\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([0.57,  # 1\n",
    "              0.04,  # 0\n",
    "              0.96,  # 2\n",
    "              0.99]) # 3\n",
    "\n",
    "B = np.array([0.35000056,  # 1\n",
    "              0.35000002,  # 0\n",
    "              0.35000098,  # 2\n",
    "              0.35000111]) # 3\n",
    "\n",
    "C = np.array([0.350000]*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When averaging model with uncorrelated model added information is only minimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "```python\n",
    "A = np.array([0.57, 0.04, 0.96, 0.99])\n",
    "\n",
    "B = np.array([0.35000056, 0.35000002, 0.35000098, 0.35000111])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.46000028  0.19500001  0.65500049  0.67000055]\n"
     ]
    }
   ],
   "source": [
    "# Arithmetic Mean\n",
    "A_B = (A + B)/2\n",
    "print(A_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.80000000e-07   1.00000000e-08   4.90000000e-07   5.55000000e-07]\n"
     ]
    }
   ],
   "source": [
    "A_C = (A + C)/2\n",
    "print(A_B-A_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.33333333  0.          0.66666667  1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Rank Averaging\n",
    "R_AB = (np.argsort(A)+np.argsort(B))/2\n",
    "print(R_AB / np.max(R_AB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How To Select Base Models?\n",
    "\n",
    "* [Forward Selection](https://en.wikipedia.org/wiki/Stepwise_regression) of base models\n",
    "* Model selection with replacement\n",
    "* Meta-algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Meta-Algorithms\n",
    "\n",
    "* Bagging\n",
    "* Boosting\n",
    "* Stacking/Blending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Every algorithm consists of two steps (<a href=\"https://stats.stackexchange.com/a/19053\">stats.stackexchange.com</a>):\n",
    "  1. Producing a distribution of **simple models** on **subsets** of the original data.\n",
    "  2. Combining the distribution of simple models into one **aggregated** model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Meta-Algorithms\n",
    "\n",
    "\n",
    "### Pros\n",
    "* Better prediction\n",
    "* More stable model\n",
    "\n",
    "### Cons\n",
    "* Slower\n",
    "* Models are non-human readeable\n",
    "* Can cause overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging\n",
    "\n",
    "https://en.wikipedia.org/wiki/Bootstrap_aggregating\n",
    "\n",
    "1. Create **random samples** (sampling uniformly and **with replacement**) of the training data set.\n",
    "2. Train a model **from each sample**.\n",
    "3. **Combine** results of these multiple classifiers using **average** (regression) or **majority voting** (classification).\n",
    "\n",
    "\n",
    "* Bagging helps to reduce the variance error.\n",
    "* Models are trained **independently**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging\n",
    "\n",
    "### Pasting\n",
    "\n",
    "* Same method as bagging, however training samples are sampled **without replacement**.\n",
    "* Set ```bootstrap=False``` in [sklearn.ensemble.BaggingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) or [sklearn.ensemble.BaggingRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/martinkersner/meta-learning-meetup/master/data/bagging.jpg\" style=\"height:45%; width:45%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bagging\n",
    "\n",
    "#### scikit-learn\n",
    "\n",
    "* [sklearn.ensemble.BaggingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)\n",
    "* [sklearn.ensemble.BaggingRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
       "            splitter='best'),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=1.0, n_estimators=500, n_jobs=1, oob_score=False,\n",
       "         random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: Hands-on Machine Learning with Scikit-Learn & Tensorflow, Chapter 7\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(random_state=random_state),\n",
    "                            # 500 base models (Decision Trees)\n",
    "                            n_estimators=500,\n",
    "                            # if True, features are randomly selected with replacement\n",
    "                            bootstrap_features=False,\n",
    "                            # if False, then using all data\n",
    "                            bootstrap=True,\n",
    "                            random_state=random_state)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Forest\n",
    "\n",
    "https://en.wikipedia.org/wiki/Random_forest\n",
    "\n",
    "* Bagging algorithm\n",
    "* Base learners are [Decision Trees](https://en.wikipedia.org/wiki/Decision_tree_learning).\n",
    "* Classification, Regression\n",
    "* De-correlation by **random sampling** (both data and features).\n",
    "* An **optimal number of trees** can be found using **cross-validation** or by observing the **out-of-bag error**.\n",
    "\n",
    "#### Out-Of-Bag Error\n",
    "\n",
    "* The mean prediction error on each training sample $X_i$, using only the trees that did not have $X_i$ in their bootstrap sample.\n",
    "* ```oob_score_``` attribute in [sklearn.ensemble.RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) when trained with ```oob_score=True```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Forest\n",
    "\n",
    "#### Training procedure\n",
    "\n",
    "1. Select a random sample (from training data) with replacement.\n",
    "2. At each node split, utilize only random subset of the features (= \"feature bagging\").\n",
    "   * If ```max_features=auto``` in [sklearn.ensemble.RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) then ${size\\_of\\_subset} = \\sqrt{number\\space of\\space features}$\n",
    "3. Repeat 1 and 2 steps until you obtain desired number of weak learners.\n",
    "4. Combine base learners for final prediction using **mode** (classification) or **mean** (regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Forest\n",
    "\n",
    "#### scikit-learn\n",
    "\n",
    "* [sklearn.ensemble.RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "* [sklearn.ensemble.RandomForestRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100,\n",
    "                             max_depth=2,\n",
    "                             random_state=random_state)\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Extremely Randomized Trees\n",
    "\n",
    "Same as [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) but **nodes are NOT split based on the most discriminative threshold**, thresholds are drawn at **random** for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule.\n",
    "\n",
    "* Decrease variance even more.\n",
    "* Bias slightly increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Extremely Randomized Trees\n",
    "\n",
    "#### scikit-learn\n",
    "\n",
    "[sklearn.ensemble.ExtraTreesClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "           oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=100,\n",
    "                           max_depth=2,\n",
    "                           random_state=0)\n",
    "\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting\n",
    "\n",
    "https://en.wikipedia.org/wiki/Boosting_(machine_learning)\n",
    "http://www.cs.princeton.edu/courses/archive/spr08/cos424/readings/Schapire2003.pdf\n",
    "\n",
    "**Boosting** is a method of turning a <u>sequence</u> of **weak learners** to one **strong learner**.\n",
    "\n",
    "* Weak learner\n",
    "  * Classifier/Regressor which can label testing examples **better than random guessing**.\n",
    "* Strong learner\n",
    "  * Classifier/Regressor that is **arbitrarily well-correlated** with the true label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting\n",
    "\n",
    "#### Properties\n",
    "* Models are trained **sequentally**.\n",
    "* Unlike bagging, **data subset creation is not random** and depends upon the performance of the previous models.\n",
    "* When weak learners are put together, they are typically weighted in some way.\n",
    "\n",
    "\n",
    "* Boosting is primarily **reducing bias**.\n",
    "* Tends to overfit the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting\n",
    "\n",
    "#### Training Procedure\n",
    "\n",
    "1. Train a weak learner on whole training dataset.\n",
    "2. Train another weak learner that will try to improve classification/regression results performed by previous weak learners.\n",
    "3. Combine all weak learners together and evaluate.\n",
    "4. Repeat steps 2-3 until you achieve desired accuracy or reach the maximum number of weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AdaBoost\n",
    "\n",
    "https://en.wikipedia.org/wiki/AdaBoost\n",
    "\n",
    "#### Properties\n",
    "\n",
    "* Any weak learner can be used (often used decision stumps).\n",
    "* Sensitive to noisy data and outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Training Procedure\n",
    "\n",
    "1. Assign weight (same for each example; $D_{1}(i) = \\frac{1}{m}$) to each training example.\n",
    "2. Train weak learner on whole training dataset.\n",
    "3. Evaluate weak learner and reweight data accordingly.\n",
    "   * Misclassified examples **gain** weight.\n",
    "   * Correctly clasified examples **lose** weight.\n",
    "4. Train another weak learner that focuses on examples that were misclassified by previous weak learner.\n",
    "5. Evaluate weak learner and update weights appropriately (as in step 3).\n",
    "6. Combine all weak learners using **weighted sum** and evaluate.\n",
    "7. Repeat steps 4 - 6 until you achieve desired accuracy or reach the maximum number of weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AdaBoost\n",
    "\n",
    "#### Reweighting\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/martinkersner/meta-learning-meetup/master/data/adaboost.png\" style=\"height: 50%; width: 50%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AdaBoost\n",
    "#### Binary Classifier\n",
    "\n",
    "* $WeakLearner = {f_{t}(x)} = \\alpha_{t} h_{t}(x)$\n",
    "* $AdaBoost_{T}(x) = sign(\\sum_{t=1}^{T}{f_{t}(x)})$\n",
    "\n",
    "Minimizing error of AdaBoost classifier at $t$-th iteration:\n",
    "\n",
    "* $E_{t} = \\sum_{i}{E[AdaBoost_{t-1}(x_i) + \\alpha_{t} h_{t}(x_i)]}$, where $E$ represents error function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AdaBoost\n",
    "#### Visualization of training\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/martinkersner/meta-learning-meetup/master/data/adaboost_training.png\" style=\"height:75%; width:75%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AdaBoost\n",
    "\n",
    "#### scikit-learn\n",
    "\n",
    "* ```base_estimator``` defines a weak learner (requires ```sample_weight``` parameter in ```fit()``` method).\n",
    "* ```n_estimator``` represents number of weak learners.\n",
    "\n",
    "#### AdaBoostClassifier\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "\n",
    "```python\n",
    "sklearn.ensemble.AdaBoostClassifier(base_estimator=None,\n",
    "                                    n_estimators=50,\n",
    "                                    learning_rate=1.0,\n",
    "                                    algorithm='SAMME.R',\n",
    "                                    random_state=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### AdaBoostRegressor\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html\n",
    "\n",
    "```python\n",
    "sklearn.ensemble.AdaBoostRegressor(base_estimator=None,\n",
    "                                   n_estimators=50,\n",
    "                                   learning_rate=1.0,\n",
    "                                   loss='linear',\n",
    "                                   random_state=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "* Trained **sequentually**\n",
    "* Classification / Regression\n",
    "* Add models to an ensemble; each model is correcting its predecessor.\n",
    "* Fit new model to the **residual errors** made by previous model.\n",
    "* Early stopping\n",
    " * Technique to estimate number of base models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# The first tree\n",
    "t1 = DecisionTreeRegressor().fit(X, y)\n",
    "\n",
    "# The second tree\n",
    "y2 = y - t1.predict(X)\n",
    "t2 = DecisionTreeRegressor().fit(X, y2)\n",
    "\n",
    "# The third tree\n",
    "y3 = y2 - t2.predict(X)\n",
    "t3 = DecisionTreeRegressor().fit(X, y3)\n",
    "```\n",
    "```python\n",
    "# Final prediction\n",
    "t = sum(t1.predict(X_new) + t2.predict(X_new) + t3.predict(X_new))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "#### Stochastic Gradient Boosting\n",
    "If `subsample` parameter is less than 1.0 sample only part of training dataset [sklearn.ensemble.GradientBoostingRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor.staged_predict)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "#### scikit-learn\n",
    "[sklearn.ensemble.GradientBoostingRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)\n",
    "\n",
    "#### Other open-source implementations\n",
    "\n",
    "* https://github.com/dmlc/xgboost\n",
    "* https://github.com/catboost/catboost\n",
    "* https://github.com/Microsoft/LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stacking (Stacked Generalization)\n",
    "\n",
    "**Training a model to combine** the predictions of several other models.\n",
    "\n",
    "1. Split dataset to $n$ folds.\n",
    "2. Train independently on each fold and predict for the others.\n",
    "3. Aggregate predictions from different folds and use them as input to another layer.\n",
    "4. If there are more layers, predictions are split and trained on $n$ folds independently again.\n",
    "\n",
    "\n",
    "* Because each layer uses the \"same\" dataset, due to incorrect data manipulation **information leak** could happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Blending\n",
    "\n",
    "* Similar to stacking, but **uses less data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "1. Split dataset to $n$ parts, where $n$ represents number of layers.\n",
    "2. Train model(s) on the first part of data and predict on the second part.\n",
    "3. Train another layer of model(s) using predictions from previous layer.\n",
    "4. Repeat step 3 until $n$ is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Open-source implementation for stacking/blending\n",
    "\n",
    "https://github.com/viisar/brew\n",
    "\n",
    "* Ensembling\n",
    "* Stacking\n",
    "* Blending\n",
    "* Ensemble Generation\n",
    "* Ensemble Pruning\n",
    "* Dynamic Classifier Selection\n",
    "* Dynamic Ensemble Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nexar\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/martinkersner/meta-learning-meetup/master/data/nexarmeata.png\" style=\"height:100;width:100%\"/>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
