{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Meta Learning\n",
    "\n",
    "## Seoul AI Meetup, September 16\n",
    "\n",
    "Martin Kersner, <m.kersner@gmail.com>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Content \n",
    "\n",
    "* Prerequsities\n",
    "  * Supervised Learning\n",
    "  * Data splitting\n",
    "  * Bias, Variance, Irreducible error\n",
    "  * Underfitting, Overfitting\n",
    "* Meta Learning\n",
    "  * discovering meta knowledge/bagging/boosting/stacking/dynamic bias selection/inductive transfer\n",
    "  * adaboost, gradient boosting, random forest\n",
    "  * Netflix, Kaggle competitions\n",
    "  * Nexar challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References \n",
    "\n",
    "* Ensembling\n",
    "  * https://mlwave.com/kaggle-ensembling-guide/\n",
    "* AdaBoost\n",
    "  * http://www.robots.ox.ac.uk/~az/lectures/cv/adaboost_matas.pdf\n",
    "  * http://www.cs.princeton.edu/courses/archive/spr08/cos424/readings/Schapire2003.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links\n",
    "\n",
    "* https://github.com/MLWave/hodor-autoML/tree/master/stacking/blend_proba\n",
    "* https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/blend_proba.py\n",
    "* https://github.com/emanuele/kaggle_pbr/blob/master/blend.py\n",
    "* https://github.com/MLWave/Kaggle-Ensemble-Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Meta Learning\n",
    "\n",
    "Meta Learning is a way of **combining models** using meta algorithms.\n",
    "\n",
    "* Pros\n",
    "  * Better prediction\n",
    "  * More stable model\n",
    "* Cons\n",
    "  * Slower\n",
    "  * Models are non-human readeable\n",
    "  * Can cause overfitting (Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mathematical Notation\n",
    "\n",
    "* $X$ data, input space\n",
    "* $Y$ labels, output space\n",
    "* $x_i$ is the feature vector of the i-th example\n",
    "* $y_i$ is label (i.e., class) for $x_i$\n",
    "* $m$ number of training examples\n",
    "* $n$ number of features\n",
    "* $D_{j}(i)$ weight of $i$-th training example for $j$-th base learner (AdaBoost)\n",
    "\n",
    "### Technical terms\n",
    "base = weak\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Learning\n",
    "\n",
    "https://en.wikipedia.org/wiki/Supervised_learning\n",
    "\n",
    "* $N$ training examples of the form $\\{\\{x_1, y_1\\},\\dotso\\{x_n, y_n\\}\\}$\n",
    "* Searching for a function $g: X \\rightarrow Y$\n",
    "\n",
    "### Classification vs Regression\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Splitting\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/martinkersner/meta-learning-meetup/master/data/train_val_test.jpeg\" style=\"width:60%; height:60%\" />\n",
    "\n",
    "* **Training** dataset is used for training.\n",
    "* **Validation** dataset is used for model evaluation.\n",
    "* **Testing** data imitate real unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Splitting\n",
    "\n",
    "### Cross-validation\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/martinkersner/meta-learning-meetup/master/data/cross_validation.png\" style=\"height:60%; width:60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalization Error\n",
    "https://en.wikipedia.org/wiki/Generalization_error\n",
    "\n",
    "Generalization error is measure of how accurately an algorithm is able to predict outcome values for previously **unseen data**.\n",
    "\n",
    "\n",
    "Generalization is composed of three parts:\n",
    "\n",
    "* Bias\n",
    "* Variance\n",
    "* Irreducible Error\n",
    "  * Due to noisiness of the data.\n",
    "  * Can be reduced by cleaning the data (not using wrong/inaccurate data points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bias, Variance\n",
    "https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\n",
    "\n",
    "* Bias\n",
    "  * Error from wrong assumptions in the learning algorithm.\n",
    "  * Can cause **underfitting**.\n",
    "  \n",
    "\n",
    "* Variance\n",
    "  * Error from sensitivity to small variations in the training set.\n",
    "  * Can cause **overfitting**.\n",
    "  \n",
    "### Bias Variance Tradeoff\n",
    "* **Increasing model complexity** lead to **increase of variance** and **reduction of bias**.\n",
    "* **Reducing model complexity** lead to **increase of bias** and **reduction of variance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"https://qph.ec.quoracdn.net/main-qimg-f9c226fe76f482855b6d46b86c76779a\" style=\"height: 50%; width: 50%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bias-Variance Tradeoff\n",
    "\n",
    "<center>\n",
    "<img src=\"http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png\" style=\"height: 50%; width: 50%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overfitting, underfitting\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ensemble\n",
    "\n",
    "Combining these rules will provide robust prediction as compared to prediction done by individual rules\n",
    "Ensemble model combines multiple ‘individual’ (diverse) models together and delivers superior prediction power.\n",
    "\n",
    "an ensemble is a supervised learning technique for combining multiple weak learners/ models to produce a strong learner. Ensemble model works better, when we ensemble models with low correlation.\n",
    "\n",
    "the random forest algorithm (having multiple CART models). It performs better compared to individual CART model by classifying a new object where each tree gives “votes” for that class and the forest chooses the classification having the most votes (over all the trees in the forest). In case of regression, it takes the average of outputs of different trees.\n",
    "\n",
    "The key to creating a powerful ensemble is model diversity. An ensemble with two techniques that are very similar in nature will perform poorly than a more diverse model set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Voting Ensembles\n",
    "\n",
    "TODO link (ensembling guide MLWave)\n",
    "\n",
    "Voting ensembled works better to ensemble low-correlated model predictions.\n",
    "\n",
    "Majority votes make most sense when the evaluation metric requires hard predictions, for instance with (multiclass-) classification accuracy.\n",
    "\n",
    "Final class is selected based on (weighted) majority voting.\n",
    "\n",
    "* Majority Voting Ensemble\n",
    "  * Hard Voting\n",
    "  * Soft Voting\n",
    "    * Predict class with the highest class probability, averaged over individual classifiers.\n",
    "    * In scikit-learn all weak classifiers need to have implmented ```predict_proba()``` method.\n",
    "* Weighted Voting Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# source: Hands-on Machine Learning with Scikit-Learn & Tensorflow, Chapter 7\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# TODO\n",
    "# voting hard\n",
    "# voting soft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Majority Voting Ensemble Example\n",
    "\n",
    "3 independent binary models (A, B, C) with accuracy 70 %.\n",
    "\n",
    "* 70 % of time correct prediction.\n",
    "* 30 % of time wrong prediction.\n",
    "\n",
    "**At least two predictions (out of three) have to be correct.**\n",
    "\n",
    "\n",
    "Voting Mechanism:\n",
    "> * A: 1\n",
    "> * B: 1\n",
    "> * C: 0\n",
    ">\n",
    "> => 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### All three are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3429999999999999\n"
     ]
    }
   ],
   "source": [
    "P3 = 0.7 * 0.7 * 0.7\n",
    "print(P3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Two are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4409999999999999\n"
     ]
    }
   ],
   "source": [
    "P2 = 3 * (0.7 * 0.7 * 0.3)\n",
    "print(P2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### One is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.189\n"
     ]
    }
   ],
   "source": [
    "P1 = 3 * (0.3 * 0.3 * 0.7)\n",
    "print(P1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### None is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.027\n"
     ]
    }
   ],
   "source": [
    "P0 = 0.3 * 0.3 * 0.3\n",
    "print(P0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "P = P0 + P1 + P2 + P3\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Voting Ensemble Example Result\n",
    "\n",
    "Most of the time (P2 ~ 44 %) the majority vote corrects an error.\n",
    "\n",
    "Prediction accuracy of majority ensembling mode will be **78.38 %** (P3 + P2) which is higher than when using models individually.\n",
    "\n",
    "Using **5** independent binary models with accuracy 70 %, accuracy of majority voting raises to **83.69 %**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Correlation\n",
    "\n",
    "* Pearson Correlation\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "For highly correlated models, majority voting enembles don't help much or not at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "GT = np.array([1,1,1,1,1,1,1,1,1,1])\n",
    "\n",
    "A  = np.array([1,1,1,1,1,1,1,1,0,0]) # 80 % accuracy\n",
    "B  = np.array([1,1,1,1,1,1,1,1,0,0]) # 80 % accuracy\n",
    "C  = np.array([1,0,1,1,1,1,1,1,0,0]) # 70 % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Accuracy with voting ensembles is still 80 %!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80000000000000004"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(A+B+C >= 2)/len(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([1,1,1,1,1,1,1,1,0,0]) # 80 % accuracy\n",
    "B = np.array([0,1,1,1,0,1,1,1,0,1]) # 70 % accuracy\n",
    "C = np.array([1,0,0,0,1,0,1,1,1,1]) # 60 % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Using highly uncorrelated models, accuracy raised to 90 %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90000000000000002"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(A+B+C >= 2)/len(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Weighted Voting Ensemble\n",
    "\n",
    "To give a better model more weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Averaging\n",
    "\n",
    "works well for a wide range of problems (both classification and regression) and metrics (AUC, squared error or logaritmic loss).\n",
    "\n",
    "Geometric mean\n",
    "\n",
    "reduces overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rank Averaging\n",
    "\n",
    "do well when evaluation metric is ranking or threshold based like AUC\n",
    "\n",
    "\n",
    "https://www.kaggle.com/cbourguignat/why-calibration-works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Historical Ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stacking, Blending\n",
    "\n",
    "### Stacked Generalization\n",
    "The basic idea behind stacked generalization is to use a pool of base classifiers, then using another classifier to combine their predictions, with the aim of reducing the generalization error.\n",
    "\n",
    "**2 fold stacking**\n",
    "\n",
    "Split the train set in 2 parts: train_a and train_b\n",
    "Fit a first-stage model on train_a and create predictions for train_b\n",
    "Fit the same model on train_b and create predictions for train_a\n",
    "Finally fit the model on the entire train set and create predictions for the test set.\n",
    "Now train a second-stage stacker model on the probabilities from the first-stage model(s).\n",
    "\n",
    "A stacker model gets more information on the problem space by using the first-stage predictions as features, than if it was trained in isolation.\n",
    "\n",
    "### Blending (= stacked ensembling)\n",
    "\n",
    "It is very close to stacked generalization, but a bit simpler and less risk of an information leak.\n",
    "\n",
    "\n",
    "### Feature weighted linear stacking\n",
    "\n",
    "Feature-weighted linear stacking stacks engineered meta-features together with model predictions. The hope is that the stacking model learns which base model is the best predictor for samples with a certain feature value. Linear algorithms are used to keep the resulting model fast and simple to inspect.\n",
    "\n",
    "\n",
    "### Quadratic linear stacking of models\n",
    "\n",
    "Stacking classifiers with regressors and vice versa\n",
    "\n",
    "### Stacking unsupervised learned features\n",
    "t-Sne\n",
    "\n",
    "### Ensemble of ensembles\n",
    "You can further optimize scores by combining multiple ensembled models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Meta-Algorithms\n",
    "\n",
    "Every algorithm consists of two steps <sup><a href=\"https://stats.stackexchange.com/a/19053\">link</a></sup>:\n",
    "  1. Producing a distribution of simple models on subsets of the original data.\n",
    "  2. Combining the distribution into one \"aggregated\" model.\n",
    "\n",
    "* Bagging\n",
    "* Boosting\n",
    "* Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Weight selection for individual base models for ensemble\n",
    "\n",
    "TODO\n",
    "\n",
    "One of the most common challenge with ensemble modeling is to find optimal weights to ensemble base models.\n",
    "\n",
    "* Same weights for each model\n",
    "* Heuristical approach\n",
    "* Use cross-validation score of base models to estimate the weight\n",
    "* [Forward Selection](https://en.wikipedia.org/wiki/Stepwise_regression) of learners\n",
    "* Model selection with replacement\n",
    "* Bagging of ensemble models\n",
    "* Explore Kaggle winning solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging (Bootstrap Aggreagting)\n",
    "\n",
    "1. Create **random samples** (sampling uniformly and with replacement) of the training data set.\n",
    "2. Build a model **from each sample**.\n",
    "3. **Combine** results of these multiple classifiers using **average** (regression) or **majority voting** (classification).\n",
    "\n",
    "\n",
    "* Bagging helps to reduce the variance error.\n",
    "* Models build **independently**.\n",
    "\n",
    "### Pasting\n",
    "Same method as bagging with, however training samples are sampled **without replacement**.\n",
    "\n",
    "```python\n",
    "bootstrap=False\n",
    "```\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/martinkersner/meta-learning-meetup/master/data/bagging.jpg\" style=\"height: 40%; width: 40%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from  sklearn.ensemble import BaggingClassifier\n",
    "from  sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "# TODO\n",
    "\n",
    "# max_features\n",
    "# bootstrap_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Forests\n",
    "\n",
    "https://en.wikipedia.org/wiki/Random_forest\n",
    "\n",
    "* Bagging\n",
    "* Weak learners are [Decision Trees](https://en.wikipedia.org/wiki/Decision_tree_learning).\n",
    "* Classification, Regression\n",
    "* De-correlation by random sampling (both data and features).\n",
    "* An optimal number of trees B can be found using cross-validation or by observing the out-of-bag error.\n",
    "\n",
    "#### Out-of-bag error (= OOB)\n",
    "\n",
    "* The mean prediction error on each training sample $X_i$, using only the trees that did not have $X_i$ in their bootstrap sample.\n",
    "* ```oob_score_``` attribute in [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) when trained with ```oob_score=True```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Forests\n",
    "\n",
    "#### Training procedure\n",
    "\n",
    "1. Select a random sample (from training data) with replacement.\n",
    "2. At each node split, utilize only random subset of the features (= \"feature beagging\").\n",
    "   * If ```max_features=auto``` in [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) then ${size\\_of\\_subset} = \\sqrt{number\\space of\\space features}$\n",
    "3. Repeat 1 and 2 steps until you obtain desired number of weak learners. (can be done in parallel)\n",
    "4. Combine weak learners for final prediction using mode (classification) or mean (regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Scikit-Learn: RandomForestClassifier\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=4,\n",
    "                           n_informative=2, n_redundant=0,\n",
    "                           random_state=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# TODO regressor\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100,\n",
    "                             max_depth=2,\n",
    "                             random_state=0)\n",
    "\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Extremely Randomized Trees\n",
    "\n",
    "Same as [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) but **nodes are not split based on the most discriminative threshold**, thresholds are drawn at **random** for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule.\n",
    "\n",
    "* Decrease variance even more.\n",
    "* Bias slightly increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Scikit-Learn: ExtraTreesClassifier\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "           oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=100,\n",
    "                           max_depth=2,\n",
    "                           random_state=0)\n",
    "\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting\n",
    "\n",
    "http://www.cs.princeton.edu/courses/archive/spr08/cos424/readings/Schapire2003.pdf\n",
    "\n",
    "**Boosting** is a method of turning set of **weak learners** to one **strong learner**.\n",
    "\n",
    "TODO\n",
    "\n",
    "* Weak learner\n",
    "  * Classifier/Regressor which can label testing examples better than random guessing.\n",
    "* Strong learner\n",
    "  * Classifier/Regressor that is arbitrarily well-correlated with the true label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting\n",
    "\n",
    "#### Properties\n",
    "* Models are build **sequentally**.\n",
    "* Unlike Bagging, **data subset creation is not random** and depends upon the performance of the previous models.\n",
    "* When weak learners are put together, they are typically weighted in some way.\n",
    "\n",
    "\n",
    "* Boosting is primarily **reducing bias**.\n",
    "* Tends to overfit the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting\n",
    "\n",
    "#### Training Procedure\n",
    "\n",
    "1. Train a weak learner on whole training dataset.\n",
    "2. Train another weak learner that will try to improve classifications/regression performed by previous weak learners.\n",
    "3. Combine all weak learners together and evaluate.\n",
    "4. Repeat steps 2-3 until you achieve desired accuracy or reach the maximum number of weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AdaBoost (= Adaptive Boosting)\n",
    "\n",
    "https://en.wikipedia.org/wiki/AdaBoost\n",
    "\n",
    "#### Properties\n",
    "\n",
    "* Any weak learner can be used (often used decision stumps)\n",
    "* Adaptive = selects only those features known to improve the predictive power of the model (TODO)\n",
    "   * Reducing dimensionality\n",
    "   * Improving execution time\n",
    "* Sensitive to noisy data and outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Training Procedure\n",
    "\n",
    "1. Assign weight (same for each example; $D_{1}(i) = \\frac{1}{m}$) to each training example.\n",
    "2. Train weak learner on whole training dataset.\n",
    "3. Evaluate weak learner and reweight data accordingly.\n",
    "   * Misclassified examples **gain** weight.\n",
    "   * Correctly clasified examples **lose** weight.\n",
    "4. Train another weak learner that focuses on examples that were misclassified by previous weak learner.\n",
    "5. Evaluate weak learner and update weights appropriately (as in step 3).\n",
    "6. Combine all weak learners using **weighted sum** and evaluate.\n",
    "7. Repeat steps 4 - 6 until you achieve desired accuracy or reach the maximum number of weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AdaBoost\n",
    "\n",
    "#### Reweighting\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/martinkersner/meta-learning-meetup/master/data/adaboost.png\" style=\"height: 50%; width: 50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AdaBoost\n",
    "#### Binary Classifier\n",
    "\n",
    "* $WeakLearner = {f_{t}(x)} = \\alpha_{t} h_{t}(x)$\n",
    "* $AdaBoost_{T}(x) = sign(\\sum_{t=1}^{T}{f_{t}(x)})$\n",
    "\n",
    "Minimizing error of AdaBoost classifier at $t$-th iteration:\n",
    "\n",
    "* $E_{t} = \\sum_{i}{E[AdaBoost_{t-1}(x_i) + \\alpha_{t} h_{t}(x_i)]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AdaBoost\n",
    "#### Visualization of training\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/martinkersner/meta-learning-meetup/master/data/adaboost_training.png\" style=\"height:80%; width:80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AdaBoost\n",
    "\n",
    "#### scikit-learn\n",
    "\n",
    "* ```base_estimator``` defines a weak learner (requires ```sample_weight``` parameter in ```fit``` method).\n",
    "* ```n_estimator``` represents number of weak learners.\n",
    "\n",
    "#### AdaBoostClassifier\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "\n",
    "```python\n",
    "sklearn.ensemble.AdaBoostClassifier(base_estimator=None,\n",
    "                                    n_estimators=50,\n",
    "                                    learning_rate=1.0,\n",
    "                                    algorithm='SAMME.R',\n",
    "                                    random_state=None)\n",
    "```\n",
    "\n",
    "#### AdaBoostRegressor\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html\n",
    "\n",
    "```python\n",
    "sklearn.ensemble.AdaBoostRegressor(base_estimator=None,\n",
    "                                   n_estimators=50,\n",
    "                                   learning_rate=1.0,\n",
    "                                   loss='linear',\n",
    "                                   random_state=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TODO face detection\n",
    "\n",
    "cascade classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Brownboost\n",
    "\n",
    "TODO\n",
    "\n",
    "### xgboost\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stacking (Stacked Generalization)\n",
    "\n",
    "Training a model to combine the **predictions** of several other models.\n",
    "\n",
    "* Stacking works in two phases:\n",
    "  1. Train several base models using all data (data splitting).\n",
    "  2. Train model to make final predictions using predictions results from trained models in the first phase.\n",
    "\n",
    "\n",
    "* Weighting of base models is learned.\n",
    "\n",
    "\n",
    "* Logistic Regression\n",
    "* Non-linear algorithms (GBM, KNN, NN, RF and ET.)\n",
    "  *  Non-linear algorithms find useful interactions between the original features and the meta-model features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nexar"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
